/** \mainpage HavoqGT Documentation

\section overview Overview


HavoqGT is a framework for 
expressing asynchronous vertex-centric graph algorithms.  It provides a visitor
interface, where actions are defined at an individual vertex level.
This code was developed at Lawrence Livermore National Laboratory.

Built in C++, the framework provides a runtime for parallel communication and 
algorithm termination detection. V0.1 is an initial release with only MPI support. 
All graph data is stored in mmaped files, using Boost.Interprocess and Memory Mapped 
(mmap) I/O. Large graphs that cannot fit in main-memory may still be processed using 
mmap as external memory. For best results, high speed Flash devices are preferred 
for external memory storage.

Copyright (c) 2013-2015, Lawrence Livermore National Security, LLC. Produced at the 
Lawrence Livermore National Laboratory. Written by <a href="http://people.llnl.gov/rpearce">Roger Pearce</a>, rpearce@llnl.gov. 
LLNL-CODE-644630. All rights reserved.

\warning This manual is under construction.  Please direct questions to Roger Pearce, rpearce@llnl.gov.

This manual is divided in the following sections:
- \subpage getting_started 
- \subpage basic_usage
- \subpage developer_guide
- \subpage publications

*/

//-----------------------------------------------------------

/** \page basic_usage Basic Usage

\section running_mpi_program Running MPI Program
Familiarize yourself with how to execute an MPI program for your system.   For this manual, we will
use <i>mpiexec</i> for launching an MPI job, but your system my require <i>srun</i>, <i>mpirun</i>, or some other method.
Contact your local system administrator if you are unfamiliar with using MPI on your system.


\section basic_usage_input Generate or Ingest Input Graph

To generate an input RMAT graph, a few parameters must be set.  As an example:

\code{.sh}
  $  mpiexec -np 4 src/generate_rmat -s 17 -o outgraph 
\endcode

This configures the following settings:
-  -np 4        -- Number of MPI processes
-  -s 17        -- the Scale of the graph (Log2 the number of vertices)
-  -o test_rmat -- The base filename for graph storage.   There will be one file per MPI rank, each with a unique name based on this string

To ingest an input graph from a simple edge list, run:
\code{.sh}
  mpiexec -np 4 src/ingest_edge_list -o outgraph [Files...]
\endcode
-  -np 4        -- Number of MPI processes
-  -o test_rmat -- The base filename for graph storage.   There will be one file per MPI rank, each with a unique name based on this string
-  [Files...]   -- Input edge list files.   Can be split over multiple files.   Each file is read independently to achieve parallel I/O

The edge list format is a simple ASCII edge list, for example:
\verbatim
 0 1
 1 2
 2 3
 3 0
\endverbatim


Once the number of MPI processes has been chosen during graph construction or ingestion, it must be set to the same value for subsequent algorithms.

\section basic_usage_bfs Run Breadth-First Search

To run BFS on a previously constructed graph, <i>test_rmat</i>, run:

\code{.sh}
 $ mpiexec -np 4 src/run_bfs -i outgraph -s 0 
\endcode

This runs BFS with the following settings:
-  -np 4       -- Number of MPI processes
-  -i outgraph -- The base filename for graph storage
-  -s 0        -- Source vertex for BFS traversal


\section basic_usage_kcore Run Triangle Counting Algorithm

To count triangles on a previously constructed graph, <i>test_rmat</i>, run:

\code{.sh}
 $  mpiexec -np 4 src/run_triangle_count test_rmat
\endcode
*/

//-----------------------------------------------------------

/** \page publications Publications
Earlier incarnations of this code, and the asynchronous visitor queue techniques, have been
presented in the following publications:

- Roger Pearce, Maya Gokhale, Nancy M. Amato, "<i>Faster Parallel Traversal of Scale Free Graphs at Extreme Scale with Vertex Delegates</i>" In Proc. Supercomputing (SC), New Orleans, LA, Nov 2014. 
Proceedings(<a href="https://parasol.tamu.edu/publications/download.php?file_id=879">pdf</a>, <a href="https://parasol.tamu.edu/publications/abstract.php?pub_id=704">abstract</a>) 
- Roger Pearce, Maya Gokhale, Nancy M. Amato, "<i>Scaling Techniques for Massive Scale-Free Graphs in Distributed (External) Memory</i>," In Proc. Int. Par. and Dist. Proc. Symp. (IPDPS), May 2013. 
Proceedings(<a href="https://parasol.tamu.edu/publications/download.php?file_id=795">pdf</a>, <a href="https://parasol.tamu.edu/publications/abstract.php?pub_id=644">abstract</a>) 
- Roger Pearce, Maya Gokhale, Nancy M. Amato, "<i>Multithreaded Asynchronous Graph Traversal for In-Memory and Semi-External Memory</i>," In Proc. Supercomputing (SC), New Orleans, LA, Nov 2010. 
Proceedings(<a href="https://parasol.tamu.edu/publications/download.php?file_id=686">pdf</a>, <a href="https://parasol.tamu.edu/publications/abstract.php?pub_id=516">abstract</a>) 


*/

